{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Data Preprocessing with Seaborn Datasets\n", "You will practice data preprocessing techniques using Seaborn datasets: **Tips**, **Flights**, and **Titanic**.\n", "We will cover:\n", "- Handling Missing Values\n", "- Feature Scaling\n", "- Encoding\n", "- Binning\n", "- Normalization\n", "\nEach step includes explanation, justification, and trade-off analysis."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Imports\n", "import seaborn as sns\n", "import pandas as pd\n", "import numpy as np\n", "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n", "import matplotlib.pyplot as plt\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load datasets\n", "tips = sns.load_dataset('tips')\n", "flights = sns.load_dataset('flights')\n", "titanic = sns.load_dataset('titanic')\n", "tips.head(), flights.head(), titanic.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Handling Missing Values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Check missing values\n", "titanic.isnull().sum()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fill missing age with median, embark_town with mode, and drop deck due to too many nulls\n", "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n", "titanic['embark_town'].fillna(titanic['embark_town'].mode()[0], inplace=True)\n", "titanic.drop(columns=['deck'], inplace=True)\n", "titanic.isnull().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Feature Scaling"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Standard scaling 'total_bill' and 'tip' from tips\n", "scaler = StandardScaler()\n", "tips[['total_bill_scaled', 'tip_scaled']] = scaler.fit_transform(tips[['total_bill', 'tip']])\n", "tips.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Encoding"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Encode 'sex' and 'embarked' in Titanic\n", "le = LabelEncoder()\n", "titanic['sex_encoded'] = le.fit_transform(titanic['sex'])\n", "titanic['embarked_encoded'] = le.fit_transform(titanic['embarked'].astype(str))\n", "titanic[['sex', 'sex_encoded', 'embarked', 'embarked_encoded']].head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Binning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Bin age into categories in Titanic\n", "bins = [0, 12, 20, 40, 60, 100]\n", "labels = ['Child', 'Teen', 'Adult', 'Middle Age', 'Senior']\n", "titanic['age_group'] = pd.cut(titanic['age'], bins=bins, labels=labels)\n", "titanic[['age', 'age_group']].head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Normalization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Normalize passenger counts in Flights dataset\n", "min_max = MinMaxScaler()\n", "flights['passengers_normalized'] = min_max.fit_transform(flights[['passengers']])\n", "flights.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Conclusion\nAll preprocessing steps have been applied. Justify your methods based on data distribution, scale requirements, and ML algorithm expectations."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}